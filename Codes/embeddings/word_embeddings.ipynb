{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a44af4b9",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5379c8f1",
   "metadata": {},
   "source": [
    "**Embedding is an important step in NLP, where a word is converted into vector.** Previously, we have seen vectorization, which involves algorithms like Bag of Words, TF-IDF and OHE. We have also seen why they are not good for most of the NLP tasks. They are sparse, doesn't catch up sequential or word relations, and also does not works well with new words. So, they can't be used if we want to attain high accuracy. That's where word embeddings comes in.\n",
    "\n",
    "Embeddings are similar to vectorization, where words are converted to vectors, but they carry word meaning, their relationship with other words in the sequence. And they are trained as well. They are very good in catching semantic relationships and so was used in lots of NLP tasks.\n",
    "\n",
    "There are many word embedding techniques, some of which are:\n",
    "- Word2Vec\n",
    "- FastText\n",
    "- Glove\n",
    "- BERT Embeddings\n",
    "etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4253ab1a",
   "metadata": {},
   "source": [
    "**Disadvantages of Word Embeddings:**\n",
    "\n",
    "- They are very good in understanding semantic relationships between words, but fails to understand contexts. Means *\"bank\"* embedding in both will be same, though they are completely different.\n",
    "\n",
    "Sentence 1: \"I drank from the river bank\"\n",
    "\n",
    "Sentence 2: \"I opened a bank account\"\n",
    "\n",
    "### So, that's why modern LLM applications uses Attention based embeddings which captures contextual information. And for capturing positions in the sentece, positional embeddings are calculated as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3bbc6",
   "metadata": {},
   "source": [
    "**Advantages of Word Embeddings:**\n",
    "\n",
    "- It is not sparse like BOW, TFIDF, etc\n",
    "- Captures semantic meaning, means *emb(king)* -> *emb(prince)* and *emb(queen)* -> *emb(princess)* are captured well with relationships well captured.\n",
    "\n",
    "\n",
    "**Additional information**\n",
    "\n",
    "- Word Embeddings are static, and doesn't change over time\n",
    "- they are the activations of the hidden layer of a NN\n",
    "- They are trained over a NN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a945979b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
